\acresetall
%%=========================================
\chapter{State-of-the-art}\label{ch:LitteratureStudy}

In this chapter, a description of the computer simulations of language evolution, from \citet{lipowska2011naming, gong2004computational, munroe2002learning, lekvam2014co} will be presented. These four were chosen because they have contributed with new ideas and represent different approaches in the field of using computational models to simulate language evolution.   

\section{Lipowska, 2011}\label{sec:Lipowska}
Lipowska made a computational simulation, which used a naming game to model a non-structured language. A non-structured language is a language that has no grammar or compositionality. The objective of Lipowska's model was to illustrate the Baldwin effect. 

\subsection{Model}
Lipowska incorporates the naming game in her model, which means that every agent is equipped with its own vocabulary, which contains all words it has heard of. Each word in the vocabulary is associated with a weight, $w_{i}$. The weight of a word represents how successful the word has been in conversations for an agent. All agents in the model are placed in a \textit{lattice}. A random agent is chosen as the speaker, and with probability $p\in$ $[0, 1]$ it chooses one of its neighbours in the lattice as the listener. If a neighbour is not chosen as a listener, the agent dies. If a neighbour is chosen as a listener, the speaker then utters a word from its vocabulary, and the listener receives that word. Which word the speaking agent utters is decided by the weight of each word relative to the sum of all weights in its vocabulary, $w_{i} / \sum_{j} w_{j}$, where $w_i$ is the weight of word $i$ and $\sum_{j} w_{j}$ is the sum of all weights of the words in the agent's vocabulary. If the agent has no words in its vocabulary, it makes one up at random. If the vocabulary of the listening agent contains the spoken word, the conversation is considered a success. If the word is not in the listener's vocabulary, the conversation has failed. 

Both the listening and the speaking agent adjust the weight of their word after a conversation. If the conversation was a success, both agents increase the weight of the word according to their respective learnability variables. The learnability variable $ l \in [0, 1]$ is an alignment strategy. Alignment is a strategy that is supposed to bring their languages closer to each other \citep{steels2012experiments}. At the end of each generation, which consists of several conversations between agents, some agents die based on a probability set by the average weight in the agent's vocabulary and its age. A high average weight in an agent's vocabulary increases its fitness, whilst an agent loses fitness when it gets older. For example, if the fitness of two agents with different ages, but with the exact same average weight over their vocabularies, is compared, the youngest of the two will have the highest fitness. A young agent with a high average weight in its vocabulary is an optimal agent. A surviving agent may breed, and if it does, the offspring inherits the learnability of its parent with a certain probability. If the offspring does not inherit its parent's learnability value, it is randomly set.

\subsection{Results and Discussion}
The experiments that were performed with a small $p$ resulted in agents that did not evolve a common language. Only small clusters of agents created common languages. The value of $p$ was slightly increased for every simulation. When $p$ increased, the size of the clusters of agents slightly increased. When $p$ reached a certain threshold, almost all agents became a part of the same cluster. For each simulation, the communication success rate, $s$, was also calculated. $s$ is defined as the fraction of all successful communications over the total number of communication attempts. When $p$ got the value of about 0.23, the communication success rate increased rapidly. 

The results of the experiment show that communication success rate and average learnability correlate. In this model, the new individuals require learning to incorporate the communal language that the generation before used. By communicating with the parents, the new agents learn the language the parents used. Children with a high learnability will learn more quickly than the others. 

According to Lipowska, the model shows that learning can direct the evolution, which indicates that the Baldwin effect has an influence on language evolution \citep[Section 4]{lipowska2011naming}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gong, 2004}\label{sec:Gong}
This section will present the word order regularity model, its results, and what was concluded in the paper by \citet{gong2004computational}. 

\subsection{Model}
Gong made a simulation of the evolution of compositional language  where agents converse about ``integrated events such as tiger is running'' \citep[Section 3]{gong2004computational}. The speaking agent makes utterances trying to explain what it perceives, while the listening agent tries to understand the utterance in accordance with partial information about the environment they are in. The language evolves from a simple holistic language into a compositional language, thereby replicating how the human language is thought to have evolved. A holistic language is a language where one utterance can be mapped to a concept, such as hunt, storm, or fire. Holistic languages are primitive and complicated sentences such as \textit{a storm is coming from the east soon we have to find shelter}, can be hard to communicate using a holistic language. A compositional language is what humans speak today this type of language allows multiple concepts to be combined easily. In Gong's simulation, a holistic rule is a rule that can be mapped to a specific concept, while compositional rules can be combined into different meanings depending on the order they are presented.    

The language in Gong's model is a set of mappings between meanings and utterances, so called M-U mapping. Meanings are represented as predicate-argument structures. The predicates are actions, such as \textit{run} and the arguments are the objects that the actions are performed upon. When several arguments are used together with a predicate, the order of the arguments decide their role. An agent's language is stored through three rules; lexical rules which are M-U mappings, syntactic rules which define the compositionality by describing the order to use lexical rules, and syntactic categories which contain sets of lexical and syntactic rules that are linked to each other. All syntactic and lexical rules have a strength which describes how likely they are to use their M-U mapping successfully. The lexical rules also have an association weight which describes how likely the rule is to be linked to the category it contains. For a more detailed explanation of Gong's compositional language, see \citep[section 3.2]{gong2011simulating}.
%For example Hunt<tiger, deer> could have two meanings. The first argument is the agent, the one that performs the predicate. The second argument is the object that agent is performing the predicate upon. In this example, it means that the tiger is hunting the deer. Predicates with identical arguments are excluded from the model for simplicity. 
\begin{comment}
The agents can store knowledge through three rules:
\begin{enumerate}%[topsep=0pt,itemsep=1ex,partopsep=1ex,parsep=1ex]
   \item \textbf{Lexical rules:} are M-U mappings. A rule is activated if the rule fully or partially matches what the speaking agent uttered. A \# indicates that any object can be used in that place. \\
   Example: Hunt<tiger, \#>, indicates that tiger is hunting any object that is inserted instead of the \#.

    \item \textbf{Syntactic rules:} are the rules that control the compositionality. These rules define the order between two lexical terms. There are four types of ordering rules that define the ordering: before a word rule, after a word rule, surround a word rule, and between a word rule. ``''<<'' is the local order before, ''>>'' is after, $\bigtriangledown$ is surround, and $\bigtriangleup$ is between'' \citep[chapter 3.2]{gong2011simulating}.\\
    Example: Category 1 (S)<< Category 2 (V), means that category 1, the subject (S), is before category 2, the verb (V).

    \item \textbf{Syntactic categories:} are a set of lexical rules and syntactic rules that is linked to each other. This makes it possible to combine several rules with the same category. In this model, the syntactic categories represent the subject, the agent is the object, and the predicate is the verb.\\
    Example: Category 1 (S): List of lexical rules: Hunt<tiger, deer>, Category 1 (S)<< Category 2 (V). This category collects all rules that are connected to the subjects (S) of an utterance. 
\end{enumerate}
\end{comment}
%If a speaker does not have any lexical rules to describe a meaning, the speaker will randomly create an utterance and store this mapping as a holistic rule. In the beginning, many such rules will be created. The agents use pattern extraction to learn rules. If an agent extract identical meanings from the same rules repeatedly, compositionality will eventually emerge. 

Gong's model uses a \textit{random communication framework} \citep[section 3.4]{gong2011simulating}. Two randomly chosen agents perform many transactions of utterances. The speaker begins by randomly selecting a meaning to produce. The speaker then goes on to activate the lexical rules and the syntactic categories that regulate the rules to form a sentence. The speaker then calculates the set of winning rules, and builds up the sentence using these rules. If the speaker does not have enough knowledge to express the meaning, then random creation of rules occur.
The listener receives the utterance from the speaker together with an \textit{environmental cue}. The cue contains partial information about the environment. The lexical rules that fully or partially match the sentence received are activated. A candidate set consisting of possible ways to understand the input gotten from the speaker is created.  
\begin{comment}
\begin{enumerate}
    \item If the cue and some set of the rules match perfectly, they are combined into a candidate set.
    
    \item If the rules and the cue do not make a perfect meaning, but the constituents of the rules match the constituents in the cue, then they are combined into meaning. Gongs example of this is as follows: The linguistic rule interpreted from the sentence is Chase<tiger, \#> and the cues meaning is Chase<tiger, sheep>. These rules do not match perfectly, but they will be combined into the meaning Chase<tiger, sheep>, which is the meaning of this candidate set.
    
    \item If none of the linguistic rules from the sentence received can be combined with the cue, then the cue becomes a candidate set. Gongs example here is: Say we have the rule Chase<tiger, sheep> from the utterance, but the cue gives the rule Fight<tiger, sheep>. These rules can not be combined, and therefore, the cue becomes the meaning.
\end{enumerate}
\end{comment}
The listener calculates the strength of each candidate set. The rules of the strongest set is then used to interpret the received sentence. If the strength of the strongest set exceeds a certain threshold, the M-U mapping is added to the listener's buffer and positive feedback is given to the speaker. Then both agents reward or retract strength from their rules according to how the communication went. It is never checked whether the agents actually ended up with the same meaning.

\subsection{Results and discussion}
In total, 20 simulations with 6000 communication rounds were performed during each simulation. The simulations were performed with a population size of 10 and 20 utterances per communication. During the first 100 rounds of communication, many holistic rules are created, but almost no compositional rules are formed. After 200 rounds, the number of holistic rules drop, while many compositional rules are created. Throughout the first rounds of communication, holistic rules are the main resource for understanding utterances and cues. Very few agents share holistic rules, so a lot of the communication results in the agents not understanding each other. When compositional rules start appearing, a clear increase in the number of successful communications is observed. Once some compositional rules become shared among the agents, almost all communications are successful. It is also observed that while the rate of successful communications rise, the average amount of meanings that each agent can make increases. That fact that the agents can express many meanings and that the agents understand each other almost every time, indicate that a compositional language has appeared. 
Gong concludes that ``given some general learning abilities, such as pattern extraction and sequential learning, a communal language showing a certain degree of systematicity can emerge in a population of individuals'' \citep[chapter 6]{gong2011simulating}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Munroe \& Cangelosi, 2002}\label{sec:Munroe}
Munroe and Cangelosi's simulation \citep{munroe2002learning} studies how learning during the lifetime of an individual affects language evolution.

\subsection{Model}
 In their simulation an agent is supposed to identify whether a mushroom is edible or not based on an 18-bit representation of the mushroom's features. The world contains three sorts of edible mushrooms and three non-edible ones. The three edible mushrooms require their own type of preparation in order for the mushroom to be edible (wash, cut, squash). An agent walks around in a grid-based world for 50 steps. In order to see that an agent could perform in different environments, each agent completed 20 different worlds during one generation. The fitness of an agent is calculated by awarding 1 point every time an agent performs the correct preparation of an edible mushroom. On some of the simulations, it was intended to simulate a learning cost. The learning cost would subtract 1 point if a non-edible mushroom was eaten. A \ac{ffnn}  was used by the agents to classify the mushrooms. A \ac{ffnn} is the simplest form of \ac{ann}, where all signals are distributed in one direction, forward. Other \acp{ann}, such as recurrent neural networks can send signals backwards in the network creating cycles \citep{jain1996artificial}.

The simulation has two stages. The first stage lasts for 300 generations with a population of 100 agents. All agents try to eat as many edible mushrooms by analysing the mushroom's features. After all agents have completed their 20 worlds, the 20 best performing agents become parents of the next generation. The 20 agents make 5 copies of themselves where 10 \% of their weights are mutated. The model was set up so that cultural variation could be simulated by applying noise before the 20 best agents were selected.

The second stage lasts for 100 generations and now the agents are allowed to communicate with each other. The 20 best performing agents are carried over to the next generation and act as teachers. 90 \% of the time the agents will not have access to the features of the mushroom, but they will receive an input from their parent telling them what kind of mushroom they are observing. The other 10 \% of the time the agents have access to both the features of the mushroom and the input from their parents before deciding what action to take. After this, the child is provided with the features of the mushroom and generates its own description of the mushroom. Using backpropagation, the child corrects its output based on the parent's description of the mushroom. Finally, the child imitates the description of the parent by only taking the parent's description as input, trying to reproduce the parent's description as output, and performs backpropagation.

\subsection{Results and discussion}
Ten experiments were ran with different population sizes. At the end of the first stage (300 generations), eight of the experiments solved the game perfectly, by avoiding all non-edible mushrooms and preparing all the edible ones correctly. The agents reached the optimal fitness of 70 at approximately generation 150.
Those eight experiments went on to the second stage of the simulation. Seven of these managed to solve the game by combining the occasional input from the environment with the linguistic input from the parents. During the second stage, the agents reached a fitness level of 70 after only 90 generations. 
Four of these experiments created compositional languages. The language created was compositional because the first symbols were always related to the action and the last symbols were associated with the type of mushroom.
%The languages consisted of symbols for the actions approach and avoid to tell the agents whether it was edible or not. The other signals were linked to the food types. 

Experiments with the cultural variation and learning cost parameters at different values were performed. The idea behind varying the cultural variation and learning cost was that it is assumed that the Baldwin effect will be strongest when the cultural variation is low and the costs of learning outweigh the benefits.

%\subsection{Conclusion}
It was concluded that a Baldwin effect was observed in these experiments. Munroe \& Cangelosi found that with a learning cost and a changing environment, some individuals learned the language quicker than others. It was also found that when the learning environment was fixed, specific behaviours got stored in the genome. When the cultural variation was set to 0, i.e. no noise when selecting the fittest agents, even the language structure itself could be built into the agent's genome.

This experiment indicates that the theory that individuals can inherit capabilities that will help learn features more easily might be valid. In order to understand exactly how these mechanisms work more research is needed.

\section{Lekvam, 2014}
Lekvam's simulation \citep{lekvam2014co} was based on the \ac{ga} framework, that was explained in section \ref{GAalgorithm}, and inspired by Lipowska's model \citep{lipowska2011naming} and Quillian's work on social networks \citep{quillinan2006social}. He also had a goal that it should be easy to add extensions to his model.

\subsection{Model} 
\citeauthor{lekvam2014co}'s model has two processes evolving at the same time, a social structure and language. The agent's goal is to acquire a social network, which is accomplished by having successful conversations with other agents. The fitness of the agent is a combination of its connections and its age using the formula
\begin{equation}
    \mathrm{fitness} = [\exp(0.02 \cdot N_\mathrm{relations}) - 1] \exp(-0.05 \cdot t),
\end{equation}
where $N_\mathrm{relations}$ is the number of connections an agent has and $t$ is the age of the agent. The more relations an agent has, the higher fitness it has, while the agent's age negatively influences its fitness. Lekvam argues that this is a good measurement of fitness because how well an agent can communicate will affect its ability to reproduce. 

When an agent acquires $z$ connections, it will stop reaching out to others, but focus on keeping the connections it has. Other agents with less than $z$ connections can still contact it, meaning that having more than $z$ connections is possible. All connections in the social network are weighted. A successful conversation increases the weight by $1.0$, while an unsuccessful one subtracts $0.5$. 

The genome of the agents consists of four genes:
\begin{enumerate}
    \item \textbf{Extraversion} is the probability of searching one layer out in the network for new friends, given that the agent has less than $z$ connections.
    
    \item \textbf{Teach child} is the probability that the parent will be the first to speak to its child. If both of the parents have high values, it is very likely that the parents will speak to the child first.
    
    \item \textbf{Lexicon limit} is the maximum size of an agent's vocabulary. If the agent learns a new word, but its vocabulary is full, the lowest weighted word is removed.
    
    \item \textbf{Speech ability} is the probability that an agent will not randomly invent a word even though it has chosen one from its vocabulary to utter. If the probability of speech ability is high, there is a high chance that the chosen word is uttered.
\end{enumerate}

When deciding which agents that will go through to the next generation, tournament selection is used. When the set of surviving agents has been selected, they breed. Each agent chooses a partner close to it in the network and then crossover is used to combine their genes. The child has a small probability to mutate. 

\subsection{Results and discussion}
The simulations were conducted with 200 generations per simulation, a population of 1225 agents per simulation, and 5 conversations per agent per generation. Seven versions of the simulation were ran. This summary will only present the results from simulation 1, which was the main simulation of the thesis. 

It was observed that the first relationship almost all newborn agents had was with their parents. This is an easy way to make connections for the agents. The agents quickly found out that reaching far out into the social network was beneficial. Since the agents' method of acquiring more than $z$ connections was to have other agents contact them, reaching far out will increase the chances of other agents reaching out to you. The population very rarely reached full consensus. Most of the time two or three languages remained when the simulation was completed. The final social network reflects this result, as the agents usually create two or three groups.

Lekvam discusses that his model might have too many flaws to conclude anything certain. Using a naming game might not be the best way of modelling signals being mapped to meaning. He also mentions that the fitness function used in the model might not be realistic enough. However, he does conclude that he believes that the methodology used has ``a great potential''.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparison of the four models}
All four language evolution models presented utilised language games. These four language game models were simplifications of the how humans communicate while maintaining the essence of why humans communicate in the model: That in order for the human species to survive, humans need to communicate and understand each other. Language games replicate how an individual is able to learn a language through having each person evolve a personal language based on communication with other people. The language game models made by \citet{lipowska2011naming} and \citet{lekvam2014co} both used a naming game. The three language games presented in the literature study were a naming game, a spatial naming game, and a signalling game. 

The spatial naming game and the signalling game incorporated word order regularity because the models studied how a compositional language could evolve, and how a population may reach a consensus over a compositional language. These models became complicated because they studied something as complex as the evolution of a compositional language. 

The two naming game models presented were simpler than the spatial naming game and the signalling game because both naming games used a holistic language. The naming game was used to study how individuals in a population could reach a consensus concerning what utterance to use to name an object. These models studied how cultural learning has an effect on language evolution, and to do that, compositional models were not needed. 

An agent’s language changed based on conversations, so how an agent decided whom to converse with had a substantial effect on how a language evolved. Gong’s model \citep{gong2011simulating} used randomly chosen speakers and listeners, so the agents did not get to decide their listener, which could have made the language evolve unnaturally. In Munroe \& Cangelosi’s model \citep{munroe2002learning}, the parents spoke to their children, which was an improvement compared to Gong’s model, but the agents did not get to choose conversational partners by themselves.

In Lipowska’s model \citep{lipowska2011naming}, the agents were set in a lattice world, with newborn agents being placed between their parents and two other agents in the lattice. This meant that every agent was directly connected to four other agents, and an agent would conduct most of its conversations with its parents while they were alive. It also meant that an agent had a set amount of edges, and lacked the ability to establish a connection to others. The edges were not weighted, meaning that all relationships between the agents were equally strong in this model. Lipowska gave all agents the ability to choose their own conversational partner, but the way a social network was represented had its limitations. 

In Lekvam’s model \citep{lekvam2014co}, which was inspired by \citet{lipowska2011naming} and \citet{quillinan2006social}, each agent could acquire new connections by contacting other agents. Agents could lose edges as well if several conversations between two agents were unsuccessful. Each edge was weighted so that not all relationships were equal in this model. Lekvam made a social network in which agents decided who to converse with. It could be someone the agent knew or it could contact a new agent. What was missing from this model was how agents decided whom to converse with, and that both agents had the same degree of connection towards one another. 

The fitness of an agent was supposed to be an image of how well that agent performed in a specific environment. In both Lekvam’s and Lipowska’s model, the fitness of an agent was supposed to reflect the agent’s ability to be understood by others. Lipowska used a fitness based on the average weight of the words in an agent’s vocabulary and its age. The average weight over the vocabulary described how well an agent was understood, but it did not describe how many agents that understood the agent. 

Lekvam used a fitness based on the number of edges of an agent and the agent’s age. The number of edges described how many other agents that understood the agent, but it did not describe to what degree the agents understood each other. In order to do that, the fitness function would have required the weights of the social network to be used. Lekvam’s fitness function did not reward having strong edges. If one agent had ten edges, with weights less than $0.1$, while another agent had eight edges with weights more than $0.8$, the agent with the strongest weights should be compensated for having strong connections, in terms of an increased fitness. Both models used the age of an agent as well, which represented that the older an agent became, the more likely it was to die. 

\citet{munroe2002learning} used a very different fitness function which was based on how well an agent acted in an environment. How well an agent performed was directly correlated to how well the agent understood the input it got from its parents, and so the fitness function reflected how an agent was able to understand other agents. The language was not used as a part of the fitness directly, a separate world was created where an agent's ability to understand language was tested. This was a clever method which tested how an agent learned the language of its parents.


%%=========================================